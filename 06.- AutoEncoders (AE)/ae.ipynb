{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OORsGuoGkiJJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\jose.lozas\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\jose.lozas\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "ERROR: Could not find a version that satisfies the requirement torch===1.6.0 (from versions: 1.7.1, 1.7.1+cpu, 1.7.1+cu101, 1.7.1+cu110, 1.8.0, 1.8.0+cpu, 1.8.0+cu101, 1.8.0+cu111, 1.8.1, 1.8.1+cpu, 1.8.1+cu101, 1.8.1+cu102, 1.8.1+cu111, 1.9.0, 1.9.0+cpu, 1.9.0+cu102, 1.9.0+cu111, 1.9.1, 1.9.1+cpu, 1.9.1+cu102, 1.9.1+cu111, 1.10.0, 1.10.0+cpu, 1.10.0+cu102, 1.10.0+cu111, 1.10.0+cu113, 1.10.1, 1.10.1+cpu, 1.10.1+cu102, 1.10.1+cu111, 1.10.1+cu113, 1.10.2, 1.10.2+cpu, 1.10.2+cu102, 1.10.2+cu111, 1.10.2+cu113, 1.11.0, 1.11.0+cpu, 1.11.0+cu113, 1.11.0+cu115, 1.12.0, 1.12.0+cpu, 1.12.0+cu113, 1.12.0+cu116, 1.12.1, 1.12.1+cpu, 1.12.1+cu113, 1.12.1+cu116, 1.13.0, 1.13.0+cpu, 1.13.0+cu116, 1.13.0+cu117, 1.13.1, 1.13.1+cpu, 1.13.1+cu116, 1.13.1+cu117, 2.0.0, 2.0.0+cpu, 2.0.0+cu117, 2.0.0+cu118, 2.0.1, 2.0.1+cpu, 2.0.1+cu117, 2.0.1+cu118, 2.1.0, 2.1.0+cpu, 2.1.0+cu118, 2.1.0+cu121, 2.1.1, 2.1.1+cpu, 2.1.1+cu118, 2.1.1+cu121, 2.1.2, 2.1.2+cpu, 2.1.2+cu118, 2.1.2+cu121)\n",
            "ERROR: No matching distribution found for torch===1.6.0\n",
            "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\jose.lozas\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\jose.lozas\\appdata\\roaming\\python\\python39\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\jose.lozas\\appdata\\roaming\\python\\python39\\site-packages)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n"
          ]
        }
      ],
      "source": [
        "pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hd7dRHyGk59L"
      },
      "source": [
        "# Importar las librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "a-h2dwXIkt5w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DI6WdEzbk-Za"
      },
      "source": [
        "# Importar el dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Xoeia18zk9jr"
      },
      "outputs": [],
      "source": [
        "movies = pd.read_csv(\"ml-1m/movies.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "users  = pd.read_csv(\"ml-1m/users.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
        "ratings  = pd.read_csv(\"ml-1m/ratings.dat\", sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KSbdp-wUlFIX"
      },
      "source": [
        "# Preparar el conjunto de entrenamiento y elconjunto de testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "70yEO3RWlBg2"
      },
      "outputs": [],
      "source": [
        "training_set = pd.read_csv(\"ml-100k/u1.base\", sep = \"\\t\", header = None)\n",
        "training_set = np.array(training_set, dtype = \"int\")\n",
        "test_set = pd.read_csv(\"ml-100k/u1.test\", sep = \"\\t\", header = None)\n",
        "test_set = np.array(test_set, dtype = \"int\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "twATLFWTlMKM"
      },
      "source": [
        "# Obtener el número de usuarios y de películas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3CGpdosSlHXu"
      },
      "outputs": [],
      "source": [
        "nb_users = int(max(max(training_set[:, 0]), max(test_set[:,0])))\n",
        "nb_movies = int(max(max(training_set[:, 1]), max(test_set[:, 1])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tSC4LbuvlT_I"
      },
      "source": [
        "# Convertir los datos en un array X[u,i] con usuarios u en fila y películas i en columna\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "VmJQ-0fDlJZ4"
      },
      "outputs": [],
      "source": [
        "def convert(data):\n",
        "    new_data = []\n",
        "    for id_user in range(1, nb_users+1):\n",
        "        id_movies = data[:, 1][data[:, 0] == id_user]\n",
        "        id_ratings = data[:, 2][data[:, 0] == id_user]\n",
        "        ratings = np.zeros(nb_movies)\n",
        "        ratings[id_movies-1] = id_ratings\n",
        "        new_data.append(list(ratings))\n",
        "    return new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Jno3ahx9lXB3"
      },
      "outputs": [],
      "source": [
        "training_set = convert(training_set)\n",
        "test_set = convert(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q7n1NZqylbCO"
      },
      "source": [
        "# Convertir los datos a tensores de Torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qTRjUdQLlYdP"
      },
      "outputs": [],
      "source": [
        "training_set = torch.FloatTensor(training_set)\n",
        "test_set = torch.FloatTensor(test_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0JAFQYBflfqd"
      },
      "source": [
        "# Crear la arquitectura de la Red Neuronal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YbDUudP_ldDe"
      },
      "outputs": [],
      "source": [
        "class SAE(nn.Module):\n",
        "    def __init__(self, ):\n",
        "        super(SAE, self).__init__()\n",
        "        self.fc1 = nn.Linear(nb_movies, 20)\n",
        "        self.fc2 = nn.Linear(20, 10)\n",
        "        self.fc3 = nn.Linear(10, 20)\n",
        "        self.fc4 = nn.Linear(20, nb_movies)\n",
        "        self.activation = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.activation(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TC6XXcLklhrU"
      },
      "outputs": [],
      "source": [
        "sae = SAE()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EqRbcjMIlo1P"
      },
      "source": [
        "# Entrenar el SAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "gy_6yKN7lkAd",
        "outputId": "921a8746-6d44-4745-99fc-bd140fcd2510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: tensor(1.7717)\n",
            "Epoch: 2, Loss: tensor(1.0965)\n",
            "Epoch: 3, Loss: tensor(1.0534)\n",
            "Epoch: 4, Loss: tensor(1.0382)\n",
            "Epoch: 5, Loss: tensor(1.0310)\n",
            "Epoch: 6, Loss: tensor(1.0265)\n",
            "Epoch: 7, Loss: tensor(1.0239)\n",
            "Epoch: 8, Loss: tensor(1.0220)\n",
            "Epoch: 9, Loss: tensor(1.0206)\n",
            "Epoch: 10, Loss: tensor(1.0196)\n",
            "Epoch: 11, Loss: tensor(1.0188)\n",
            "Epoch: 12, Loss: tensor(1.0185)\n",
            "Epoch: 13, Loss: tensor(1.0178)\n",
            "Epoch: 14, Loss: tensor(1.0173)\n",
            "Epoch: 15, Loss: tensor(1.0172)\n",
            "Epoch: 16, Loss: tensor(1.0169)\n",
            "Epoch: 17, Loss: tensor(1.0168)\n",
            "Epoch: 18, Loss: tensor(1.0166)\n",
            "Epoch: 19, Loss: tensor(1.0161)\n",
            "Epoch: 20, Loss: tensor(1.0161)\n",
            "Epoch: 21, Loss: tensor(1.0158)\n",
            "Epoch: 22, Loss: tensor(1.0161)\n",
            "Epoch: 23, Loss: tensor(1.0159)\n",
            "Epoch: 24, Loss: tensor(1.0159)\n",
            "Epoch: 25, Loss: tensor(1.0156)\n",
            "Epoch: 26, Loss: tensor(1.0156)\n",
            "Epoch: 27, Loss: tensor(1.0153)\n",
            "Epoch: 28, Loss: tensor(1.0150)\n",
            "Epoch: 29, Loss: tensor(1.0131)\n",
            "Epoch: 30, Loss: tensor(1.0116)\n",
            "Epoch: 31, Loss: tensor(1.0092)\n",
            "Epoch: 32, Loss: tensor(1.0088)\n",
            "Epoch: 33, Loss: tensor(1.0050)\n",
            "Epoch: 34, Loss: tensor(1.0057)\n",
            "Epoch: 35, Loss: tensor(1.0017)\n",
            "Epoch: 36, Loss: tensor(0.9996)\n",
            "Epoch: 37, Loss: tensor(0.9972)\n",
            "Epoch: 38, Loss: tensor(0.9963)\n",
            "Epoch: 39, Loss: tensor(0.9917)\n",
            "Epoch: 40, Loss: tensor(0.9926)\n",
            "Epoch: 41, Loss: tensor(0.9900)\n",
            "Epoch: 42, Loss: tensor(0.9890)\n",
            "Epoch: 43, Loss: tensor(0.9840)\n",
            "Epoch: 44, Loss: tensor(0.9810)\n",
            "Epoch: 45, Loss: tensor(0.9834)\n",
            "Epoch: 46, Loss: tensor(0.9836)\n",
            "Epoch: 47, Loss: tensor(0.9784)\n",
            "Epoch: 48, Loss: tensor(0.9794)\n",
            "Epoch: 49, Loss: tensor(0.9747)\n",
            "Epoch: 50, Loss: tensor(0.9761)\n",
            "Epoch: 51, Loss: tensor(0.9744)\n",
            "Epoch: 52, Loss: tensor(0.9735)\n",
            "Epoch: 53, Loss: tensor(0.9713)\n",
            "Epoch: 54, Loss: tensor(0.9727)\n",
            "Epoch: 55, Loss: tensor(0.9710)\n",
            "Epoch: 56, Loss: tensor(0.9673)\n",
            "Epoch: 57, Loss: tensor(0.9715)\n",
            "Epoch: 58, Loss: tensor(0.9776)\n",
            "Epoch: 59, Loss: tensor(0.9734)\n",
            "Epoch: 60, Loss: tensor(0.9711)\n",
            "Epoch: 61, Loss: tensor(0.9710)\n",
            "Epoch: 62, Loss: tensor(0.9818)\n",
            "Epoch: 63, Loss: tensor(0.9758)\n",
            "Epoch: 64, Loss: tensor(0.9715)\n",
            "Epoch: 65, Loss: tensor(0.9770)\n",
            "Epoch: 66, Loss: tensor(0.9755)\n",
            "Epoch: 67, Loss: tensor(0.9692)\n",
            "Epoch: 68, Loss: tensor(0.9691)\n",
            "Epoch: 69, Loss: tensor(0.9663)\n",
            "Epoch: 70, Loss: tensor(0.9645)\n",
            "Epoch: 71, Loss: tensor(0.9621)\n",
            "Epoch: 72, Loss: tensor(0.9622)\n",
            "Epoch: 73, Loss: tensor(0.9624)\n",
            "Epoch: 74, Loss: tensor(0.9596)\n",
            "Epoch: 75, Loss: tensor(0.9587)\n",
            "Epoch: 76, Loss: tensor(0.9550)\n",
            "Epoch: 77, Loss: tensor(0.9535)\n",
            "Epoch: 78, Loss: tensor(0.9546)\n",
            "Epoch: 79, Loss: tensor(0.9542)\n",
            "Epoch: 80, Loss: tensor(0.9526)\n",
            "Epoch: 81, Loss: tensor(0.9505)\n",
            "Epoch: 82, Loss: tensor(0.9507)\n",
            "Epoch: 83, Loss: tensor(0.9479)\n",
            "Epoch: 84, Loss: tensor(0.9505)\n",
            "Epoch: 85, Loss: tensor(0.9481)\n",
            "Epoch: 86, Loss: tensor(0.9519)\n",
            "Epoch: 87, Loss: tensor(0.9506)\n",
            "Epoch: 88, Loss: tensor(0.9544)\n",
            "Epoch: 89, Loss: tensor(0.9506)\n",
            "Epoch: 90, Loss: tensor(0.9504)\n",
            "Epoch: 91, Loss: tensor(0.9483)\n",
            "Epoch: 92, Loss: tensor(0.9465)\n",
            "Epoch: 93, Loss: tensor(0.9469)\n",
            "Epoch: 94, Loss: tensor(0.9454)\n",
            "Epoch: 95, Loss: tensor(0.9435)\n",
            "Epoch: 96, Loss: tensor(0.9464)\n",
            "Epoch: 97, Loss: tensor(0.9442)\n",
            "Epoch: 98, Loss: tensor(0.9451)\n",
            "Epoch: 99, Loss: tensor(0.9426)\n",
            "Epoch: 100, Loss: tensor(0.9423)\n",
            "Epoch: 101, Loss: tensor(0.9396)\n",
            "Epoch: 102, Loss: tensor(0.9394)\n",
            "Epoch: 103, Loss: tensor(0.9402)\n",
            "Epoch: 104, Loss: tensor(0.9410)\n",
            "Epoch: 105, Loss: tensor(0.9382)\n",
            "Epoch: 106, Loss: tensor(0.9399)\n",
            "Epoch: 107, Loss: tensor(0.9433)\n",
            "Epoch: 108, Loss: tensor(0.9476)\n",
            "Epoch: 109, Loss: tensor(0.9445)\n",
            "Epoch: 110, Loss: tensor(0.9424)\n",
            "Epoch: 111, Loss: tensor(0.9397)\n",
            "Epoch: 112, Loss: tensor(0.9403)\n",
            "Epoch: 113, Loss: tensor(0.9388)\n",
            "Epoch: 114, Loss: tensor(0.9382)\n",
            "Epoch: 115, Loss: tensor(0.9362)\n",
            "Epoch: 116, Loss: tensor(0.9370)\n",
            "Epoch: 117, Loss: tensor(0.9345)\n",
            "Epoch: 118, Loss: tensor(0.9362)\n",
            "Epoch: 119, Loss: tensor(0.9341)\n",
            "Epoch: 120, Loss: tensor(0.9367)\n",
            "Epoch: 121, Loss: tensor(0.9342)\n",
            "Epoch: 122, Loss: tensor(0.9346)\n",
            "Epoch: 123, Loss: tensor(0.9341)\n",
            "Epoch: 124, Loss: tensor(0.9356)\n",
            "Epoch: 125, Loss: tensor(0.9308)\n",
            "Epoch: 126, Loss: tensor(0.9333)\n",
            "Epoch: 127, Loss: tensor(0.9301)\n",
            "Epoch: 128, Loss: tensor(0.9319)\n",
            "Epoch: 129, Loss: tensor(0.9289)\n",
            "Epoch: 130, Loss: tensor(0.9305)\n",
            "Epoch: 131, Loss: tensor(0.9281)\n",
            "Epoch: 132, Loss: tensor(0.9299)\n",
            "Epoch: 133, Loss: tensor(0.9285)\n",
            "Epoch: 134, Loss: tensor(0.9297)\n",
            "Epoch: 135, Loss: tensor(0.9272)\n",
            "Epoch: 136, Loss: tensor(0.9284)\n",
            "Epoch: 137, Loss: tensor(0.9267)\n",
            "Epoch: 138, Loss: tensor(0.9275)\n",
            "Epoch: 139, Loss: tensor(0.9253)\n",
            "Epoch: 140, Loss: tensor(0.9269)\n",
            "Epoch: 141, Loss: tensor(0.9252)\n",
            "Epoch: 142, Loss: tensor(0.9266)\n",
            "Epoch: 143, Loss: tensor(0.9252)\n",
            "Epoch: 144, Loss: tensor(0.9297)\n",
            "Epoch: 145, Loss: tensor(0.9246)\n",
            "Epoch: 146, Loss: tensor(0.9253)\n",
            "Epoch: 147, Loss: tensor(0.9240)\n",
            "Epoch: 148, Loss: tensor(0.9251)\n",
            "Epoch: 149, Loss: tensor(0.9232)\n",
            "Epoch: 150, Loss: tensor(0.9245)\n",
            "Epoch: 151, Loss: tensor(0.9224)\n",
            "Epoch: 152, Loss: tensor(0.9233)\n",
            "Epoch: 153, Loss: tensor(0.9218)\n",
            "Epoch: 154, Loss: tensor(0.9231)\n",
            "Epoch: 155, Loss: tensor(0.9220)\n",
            "Epoch: 156, Loss: tensor(0.9227)\n",
            "Epoch: 157, Loss: tensor(0.9212)\n",
            "Epoch: 158, Loss: tensor(0.9224)\n",
            "Epoch: 159, Loss: tensor(0.9206)\n",
            "Epoch: 160, Loss: tensor(0.9218)\n",
            "Epoch: 161, Loss: tensor(0.9204)\n",
            "Epoch: 162, Loss: tensor(0.9210)\n",
            "Epoch: 163, Loss: tensor(0.9199)\n",
            "Epoch: 164, Loss: tensor(0.9208)\n",
            "Epoch: 165, Loss: tensor(0.9198)\n",
            "Epoch: 166, Loss: tensor(0.9205)\n",
            "Epoch: 167, Loss: tensor(0.9195)\n",
            "Epoch: 168, Loss: tensor(0.9206)\n",
            "Epoch: 169, Loss: tensor(0.9187)\n",
            "Epoch: 170, Loss: tensor(0.9198)\n",
            "Epoch: 171, Loss: tensor(0.9194)\n",
            "Epoch: 172, Loss: tensor(0.9198)\n",
            "Epoch: 173, Loss: tensor(0.9186)\n",
            "Epoch: 174, Loss: tensor(0.9194)\n",
            "Epoch: 175, Loss: tensor(0.9180)\n",
            "Epoch: 176, Loss: tensor(0.9188)\n",
            "Epoch: 177, Loss: tensor(0.9177)\n",
            "Epoch: 178, Loss: tensor(0.9183)\n",
            "Epoch: 179, Loss: tensor(0.9172)\n",
            "Epoch: 180, Loss: tensor(0.9181)\n",
            "Epoch: 181, Loss: tensor(0.9191)\n",
            "Epoch: 182, Loss: tensor(0.9202)\n",
            "Epoch: 183, Loss: tensor(0.9174)\n",
            "Epoch: 184, Loss: tensor(0.9213)\n",
            "Epoch: 185, Loss: tensor(0.9199)\n",
            "Epoch: 186, Loss: tensor(0.9176)\n",
            "Epoch: 187, Loss: tensor(0.9167)\n",
            "Epoch: 188, Loss: tensor(0.9168)\n",
            "Epoch: 189, Loss: tensor(0.9164)\n",
            "Epoch: 190, Loss: tensor(0.9165)\n",
            "Epoch: 191, Loss: tensor(0.9159)\n",
            "Epoch: 192, Loss: tensor(0.9159)\n",
            "Epoch: 193, Loss: tensor(0.9155)\n",
            "Epoch: 194, Loss: tensor(0.9156)\n",
            "Epoch: 195, Loss: tensor(0.9151)\n",
            "Epoch: 196, Loss: tensor(0.9156)\n",
            "Epoch: 197, Loss: tensor(0.9154)\n",
            "Epoch: 198, Loss: tensor(0.9157)\n",
            "Epoch: 199, Loss: tensor(0.9146)\n",
            "Epoch: 200, Loss: tensor(0.9157)\n"
          ]
        }
      ],
      "source": [
        "nb_epoch = 200\n",
        "for epoch in range(1, nb_epoch+1):\n",
        "    train_loss = 0\n",
        "    s = 0.\n",
        "    for id_user in range(nb_users):\n",
        "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "        target = input.clone()\n",
        "        if torch.sum(target.data > 0) > 0:\n",
        "            output = sae.forward(input)\n",
        "            target.require_grad = False\n",
        "            output[target == 0] = 0\n",
        "            loss = criterion(output, target)\n",
        "            # la media no es sobre todas las películas, sino sobre las que realmente ha valorado\n",
        "            mean_corrector = nb_movies/float(torch.sum(target.data > 0)+1e-10) \n",
        "            loss.backward()\n",
        "            train_loss += np.sqrt(loss.data*mean_corrector) ## sum(errors) / n_pelis_valoradas\n",
        "            s += 1.\n",
        "            optimizer.step()\n",
        "    print(\"Epoch: \"+str(epoch)+\", Loss: \"+str(train_loss/s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ENHGLCyiluJn"
      },
      "source": [
        "# Evaluar el conjunto de test en nuestro SAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "UAL0ZZOhlruw"
      },
      "outputs": [],
      "source": [
        "test_loss = 0\n",
        "s = 0.\n",
        "for id_user in range(nb_users):\n",
        "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
        "    target = Variable(test_set[id_user]).unsqueeze(0)\n",
        "    if torch.sum(target.data > 0) > 0:\n",
        "        output = sae.forward(input)\n",
        "        target.require_grad = False\n",
        "        output[target == 0] = 0\n",
        "        loss = criterion(output, target)\n",
        "        # la media no es sobre todas las películas, sino sobre las que realmente ha valorado\n",
        "        mean_corrector = nb_movies/float(torch.sum(target.data > 0)+1e-10) \n",
        "        test_loss += np.sqrt(loss.data*mean_corrector) ## sum(errors) / n_pelis_valoradas\n",
        "        s += 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "Quy4q1UFlwfF",
        "outputId": "dbece7af-feb4-4943-aaf0-d4b9f8fc6164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: tensor(0.9605)\n"
          ]
        }
      ],
      "source": [
        "print(\"Test Loss: \"+str(test_loss/s))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ae.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
